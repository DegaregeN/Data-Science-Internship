{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing of necessary libraries\n",
    "\n",
    "import pandas as pd  \n",
    "import numpy as np  \n",
    "import seaborn as sns \n",
    "import pyreadstat \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from itertools import cycle\n",
    "import time as time\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target variable\n",
    "target_column = 'Cause pph'\n",
    "X = df.drop(target_column, axis=1)  # Features\n",
    "y = df[target_column]               # Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the target variable\n",
    "# Define colors for each class\n",
    "colors = ['red', 'yellow', 'orange', 'green']\n",
    "\n",
    "# Count frequencies of each class\n",
    "unique_classes, class_counts = np.unique(y, return_counts=True)\n",
    "\n",
    "# Plot the bar graph with different colors\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(unique_classes, class_counts, color=colors)\n",
    "\n",
    "plt.xlabel('Target Variable')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Target Variable')\n",
    "plt.grid(axis='y')\n",
    "plt.xticks(unique_classes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SMOTE to balance the dataset class variables \n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define colors for each class\n",
    "colors = ['red', 'yellow', 'orange', 'green']\n",
    "\n",
    "# Count frequencies of each class\n",
    "unique_classes, class_counts = np.unique(y_resampled, return_counts=True)\n",
    "\n",
    "# Plot the bar graph with different colors\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(unique_classes, class_counts, color=colors)\n",
    "\n",
    "plt.xlabel('Target Variable(Anaemia Level)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Target Variable(Anaemia Level) after SMOTE')\n",
    "plt.grid(axis='y')\n",
    "plt.xticks(unique_classes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_resampled, y_resampled, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your feature columns (example given, excluding the target column 'Anemia level')\n",
    "feature_columns = ['Was labor augmented', 'Mode of delivery', 'Pretreatment bp systolic', 'Pretreatment bp diastolic', 'Complications in previous pregnancy',\n",
    "                   'Age', 'Parity of index pregnanacy', 'Episiotomy', 'Experience seizures', 'Week of gestation',\n",
    "                   'Delayed arrival to health facility', 'Delay in correct diagnosis', 'Weight of newborn at birth', 'Duration of labour in hour',\n",
    "                   'Woman / baby was referred from another facility', 'Region/Province','Urban/rural']\n",
    "\n",
    "# Ensure X_train is a DataFrame with the correct number of columns\n",
    "X_train = pd.DataFrame(X_train, columns=feature_columns)\n",
    "\n",
    "# Calculate mutual information scores\n",
    "mi = mutual_info_classif(X_train, y_train)\n",
    "\n",
    "# Convert mutual information scores to a Pandas Series\n",
    "mi = pd.Series(mi)\n",
    "\n",
    "# Label the Series with feature names\n",
    "mi.index = X_train.columns\n",
    "\n",
    "# Sort the Series\n",
    "mi_sorted = mi.sort_values(ascending=False)\n",
    "\n",
    "# display the sorted Series\n",
    "mi_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the sorted Series with different color palettes\n",
    "palettes = sns.color_palette('tab10', n_colors=len(mi_sorted))\n",
    "plt.figure(figsize=(12, 8))\n",
    "mi_sorted.plot(kind='bar', color=palettes)\n",
    "plt.title('Mutual Information Scores of Features')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Mutual Information Score')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the RandomForestClassifier\n",
    "rf_clf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "\n",
    "# Train the model on the standardized training data\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_val_pred = rf_clf.predict(X_val)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "val_report = classification_report(y_val, y_val_pred)\n",
    "\n",
    "print(f'Validation Accuracy: {val_accuracy:.4f}')\n",
    "print('Validation Classification Report:')\n",
    "print(val_report)\n",
    "\n",
    "# Optionally, evaluate the model on the test set\n",
    "y_test_pred = rf_clf.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_report = classification_report(y_test, y_test_pred)\n",
    "\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')\n",
    "print('Test Classification Report:')\n",
    "print(test_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the validation set\n",
    "y_val_pred = rf_clf.predict(X_val)\n",
    "\n",
    "# Calculate the confusion matrix for the validation set\n",
    "val_cm = confusion_matrix(y_val, y_val_pred)\n",
    "\n",
    "# Predict on the test set\n",
    "y_test_pred = rf_clf.predict(X_test)\n",
    "\n",
    "# Calculate the confusion matrix for the test set\n",
    "test_cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "# Define class names\n",
    "class_names = ['Uterine atony', 'Trauma', 'Retained placenta', 'Cloting disorder']\n",
    "\n",
    "# Set up the matplotlib figure and subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Plot the confusion matrix for the validation set\n",
    "sns.heatmap(val_cm, annot=True, fmt='d', cmap=\"viridis\", cbar=False, ax=axes[0],\n",
    "            xticklabels=class_names, yticklabels=class_names, annot_kws={\"size\": 16})\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "axes[0].set_title('Confusion Matrix for Validation Set')\n",
    "\n",
    "# Plot the confusion matrix for the test set\n",
    "sns.heatmap(test_cm, annot=True, fmt='d', cmap=\"plasma\", cbar=False, ax=axes[1],\n",
    "            xticklabels=class_names, yticklabels=class_names,annot_kws={\"size\": 16})\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "axes[1].set_title('Confusion Matrix for Test Set')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [1, 2, 3, 4]\n",
    "\n",
    "# Predict probabilities for each class on the validation set\n",
    "y_score = rf_clf.predict_proba(X_test)\n",
    "\n",
    "# Binarize the true labels for multi-class ROC\n",
    "y_test_bin = label_binarize(y_test, classes=classes)\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "plt.figure(figsize=(10, 7))\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(len(classes)):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Plot ROC curves for each class\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'green'])\n",
    "for i, color in zip(range(len(classes)), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "             label=f'ROC curve (class {classes[i]}) (AUC = {roc_auc[i]:.3f})')\n",
    "\n",
    "# Plot random guess line\n",
    "plt.plot([0, 1], [0, 1], color='red', linestyle='--', label='Random Guess')\n",
    "\n",
    "# Calculate and plot the macro-average ROC curve and AUC\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(len(classes))]))\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(len(classes)):\n",
    "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "mean_tpr /= len(classes)\n",
    "macro_auc = auc(all_fpr, mean_tpr)\n",
    "\n",
    "plt.plot(all_fpr, mean_tpr, color='navy', linestyle='-', linewidth=2,\n",
    "         label=f'Macro-average ROC curve (AUC = {macro_auc:.3f})')\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves for Random Forest (Anaemia Level = 4 Classes)')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Decision Tree classifier\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_val_pred = dt.predict(X_val)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "val_report = classification_report(y_val, y_val_pred)\n",
    "\n",
    "print(f'Validation Accuracy: {val_accuracy:.4f}')\n",
    "print('Validation Classification Report:')\n",
    "print(val_report)\n",
    "\n",
    "# Optionally, evaluate the model on the test set\n",
    "y_test_pred = dt.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_report = classification_report(y_test, y_test_pred)\n",
    "\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')\n",
    "print('Test Classification Report:')\n",
    "print(test_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the confusion matrix for the validation set\n",
    "val_cm = confusion_matrix(y_val, y_val_pred)\n",
    "\n",
    "# Calculate the confusion matrix for the test set\n",
    "test_cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "# Set up the matplotlib figure and subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Plot the confusion matrix for the validation set\n",
    "sns.heatmap(val_cm, annot=True, fmt='d', cmap=\"inferno\", cbar=False, ax=axes[0],\n",
    "            xticklabels=class_names, yticklabels=class_names, annot_kws={\"size\": 16})  # Adjust fontsize here\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "axes[0].set_title('Confusion Matrix for Validation Set of decision tree Classifier')\n",
    "\n",
    "# Plot the confusion matrix for the test set\n",
    "sns.heatmap(test_cm, annot=True, fmt='d', cmap=\"magma\", cbar=False, ax=axes[1],\n",
    "            xticklabels=class_names, yticklabels=class_names, annot_kws={\"size\": 16})  # Adjust fontsize here\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "axes[1].set_title('Confusion Matrix for Test Set of decision tree Classifier')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict probabilities for each class on the test set\n",
    "y_score = dt.predict_proba(X_test)\n",
    "\n",
    "# Compute ROC curve and ROC area for each class\n",
    "plt.figure(figsize=(10, 7))\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(4):  # There are 4 classes (1, 2, 3, 4)\n",
    "    # Binarize the true labels for class i\n",
    "    y_test_bin = label_binarize(y_test, classes=[1, 2, 3, 4])[:, i]\n",
    "    # Compute ROC curve and ROC area\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_bin, y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Plot ROC curves for each class\n",
    "colors = cycle(['aqua', 'darkorange', 'cornflowerblue', 'green'])\n",
    "for i, color in zip(range(4), colors):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "             label=f'ROC curve (class {i+1}) (AUC = {roc_auc[i]:.2f})')\n",
    "\n",
    "# Plot random guess line\n",
    "plt.plot([0, 1], [0, 1], color='red', linestyle='--', label='Random Guess')\n",
    "\n",
    "# Calculate and plot the macro-average ROC curve and AUC\n",
    "all_fpr = np.unique(np.concatenate([fpr[i] for i in range(len(classes))]))\n",
    "mean_tpr = np.zeros_like(all_fpr)\n",
    "for i in range(len(classes)):\n",
    "    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])\n",
    "mean_tpr /= len(classes)\n",
    "macro_auc = auc(all_fpr, mean_tpr)\n",
    "\n",
    "plt.plot(all_fpr, mean_tpr, color='navy', linestyle='-', linewidth=2,\n",
    "         label=f'Macro-average ROC curve (AUC = {macro_auc:.2f})')\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves for Decision Tree (Anaemia Level = 4 Classes)')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Train Logistic Regression classifier\n",
    "logreg = LogisticRegression(random_state=42, max_iter=1000)\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_val_pred = logreg.predict(X_val)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "val_report = classification_report(y_val, y_val_pred)\n",
    "\n",
    "print(f'Logistic Regression - Validation Accuracy: {val_accuracy:.4f}')\n",
    "print('Logistic Regression - Validation Classification Report:')\n",
    "print(val_report)\n",
    "\n",
    "# Optionally, evaluate the model on the test set\n",
    "y_test_pred = logreg.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_report = classification_report(y_test, y_test_pred)\n",
    "\n",
    "print(f'Logistic Regression - Test Accuracy: {test_accuracy:.4f}')\n",
    "print('Logistic Regression - Test Classification Report:')\n",
    "print(test_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Train Naive Bayes classifier\n",
    "nb = GaussianNB()\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_val_pred = nb.predict(X_val)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "val_report = classification_report(y_val, y_val_pred)\n",
    "\n",
    "print(f'Naive Bayes - Validation Accuracy: {val_accuracy:.4f}')\n",
    "print('Naive Bayes - Validation Classification Report:')\n",
    "print(val_report)\n",
    "\n",
    "# Optionally, evaluate the model on the test set\n",
    "y_test_pred = nb.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_report = classification_report(y_test, y_test_pred)\n",
    "\n",
    "print(f'Naive Bayes - Test Accuracy: {test_accuracy:.4f}')\n",
    "print('Naive Bayes - Test Classification Report:')\n",
    "print(test_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Train SVM classifier\n",
    "svm = SVC(random_state=42)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_val_pred = svm.predict(X_val)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "val_report = classification_report(y_val, y_val_pred)\n",
    "\n",
    "print(f'SVM - Validation Accuracy: {val_accuracy:.4f}')\n",
    "print('SVM - Validation Classification Report:')\n",
    "print(val_report)\n",
    "\n",
    "# Optionally, evaluate the model on the test set\n",
    "y_test_pred = svm.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_report = classification_report(y_test, y_test_pred)\n",
    "\n",
    "print(f'SVM - Test Accuracy: {test_accuracy:.4f}')\n",
    "print('SVM - Test Classification Report:')\n",
    "print(test_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Train Random Forest classifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_val_pred = rf.predict(X_val)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "val_report = classification_report(y_val, y_val_pred)\n",
    "\n",
    "print(f'Random Forest - Validation Accuracy: {val_accuracy:.4f}')\n",
    "print('Random Forest - Validation Classification Report:')\n",
    "print(val_report)\n",
    "\n",
    "# Optionally, evaluate the model on the test set\n",
    "y_test_pred = rf.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_report = classification_report(y_test, y_test_pred)\n",
    "\n",
    "print(f'Random Forest - Test Accuracy: {test_accuracy:.4f}')\n",
    "print('Random Forest - Test Classification Report:')\n",
    "print(test_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Train KNN classifier\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the validation set\n",
    "y_val_pred = knn.predict(X_val)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "val_report = classification_report(y_val, y_val_pred)\n",
    "\n",
    "print(f'KNN - Validation Accuracy: {val_accuracy:.4f}')\n",
    "print('KNN - Validation Classification Report:')\n",
    "print(val_report)\n",
    "\n",
    "# Optionally, evaluate the model on the test set\n",
    "y_test_pred = knn.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_report = classification_report(y_test, y_test_pred)\n",
    "\n",
    "print(f'KNN - Test Accuracy: {test_accuracy:.4f}')\n",
    "print('KNN - Test Classification Report:')\n",
    "print(test_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# List of models to evaluate\n",
    "models = {\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Naive Bayes': GaussianNB(),\n",
    "    'SVM': SVC(random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'KNN': KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "# Initialize a dictionary to store performance metrics\n",
    "performance_metrics = {\n",
    "    'Accuracy': [],\n",
    "    'Precision': [],\n",
    "    'Recall': [],\n",
    "    'F1 Score': []\n",
    "}\n",
    "\n",
    "# Iterate over each model to train, predict, and collect metrics\n",
    "for name, model in models.items():\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    precision = precision_score(y_test, y_test_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_test_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_test_pred, average='weighted')\n",
    "    \n",
    "    # Store metrics\n",
    "    performance_metrics['Accuracy'].append(accuracy)\n",
    "    performance_metrics['Precision'].append(precision)\n",
    "    performance_metrics['Recall'].append(recall)\n",
    "    performance_metrics['F1 Score'].append(f1)\n",
    "\n",
    "# Convert to numpy array for plotting\n",
    "metrics_array = np.array([performance_metrics['Accuracy'],\n",
    "                          performance_metrics['Precision'],\n",
    "                          performance_metrics['Recall'],\n",
    "                          performance_metrics['F1 Score']])\n",
    "\n",
    "# Create the grouped bar plot\n",
    "model_names = list(models.keys())\n",
    "metric_names = list(performance_metrics.keys())\n",
    "n_metrics = len(metric_names)\n",
    "n_models = len(model_names)\n",
    "\n",
    "# Set up the plot\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Define bar width and positions\n",
    "bar_width = 0.2\n",
    "indices = np.arange(n_models)\n",
    "\n",
    "# Plot bars for each metric\n",
    "for i, metric in enumerate(metric_names):\n",
    "    ax.bar(indices + i * bar_width, metrics_array[i], bar_width, label=metric)\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel('Classifiers')\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_title('Grouped Bar Plot of Classification Performances')\n",
    "ax.set_xticks(indices + bar_width * (n_metrics - 1) / 2)\n",
    "ax.set_xticklabels(model_names, rotation=45)\n",
    "ax.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
